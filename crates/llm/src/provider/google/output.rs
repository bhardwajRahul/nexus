use serde::{Deserialize, Serialize};

use crate::messages::{ChatChoice, ChatCompletionResponse, ChatMessage, ChatRole, Model, ObjectType, Usage};

/// Reason why the model stopped generating tokens.
///
/// Indicates the completion status of the generation.
#[derive(Debug, Deserialize, PartialEq)]
pub(super) enum FinishReason {
    /// Natural stop point was reached.
    #[serde(rename = "STOP")]
    Stop,
    /// Maximum number of tokens was reached.
    #[serde(rename = "MAX_TOKENS")]
    MaxTokens,
    /// Content was filtered due to safety concerns.
    #[serde(rename = "SAFETY")]
    Safety,
    /// Content was filtered due to recitation concerns (potential plagiarism).
    #[serde(rename = "RECITATION")]
    Recitation,
    /// Explicitly marked as "OTHER" by Google.
    #[serde(rename = "OTHER")]
    Other,
    /// Any other finish reason not yet known.
    /// Captures the actual string value for forward compatibility.
    #[serde(untagged)]
    Unknown(String),
}

/// Represents content in a conversation with Gemini.
///
/// Used for both input (user/system messages) and output (model responses).
/// This type is shared between input and output as Google uses the same structure for both.
#[derive(Debug, Deserialize, Serialize)]
pub(super) struct GoogleContent {
    /// The parts that compose this content.
    ///
    /// A content can have multiple parts, each containing different types of data
    /// (text, images, etc.), though currently we only support text.
    pub(super) parts: Vec<GooglePart>,

    /// The role of the content creator.
    ///
    /// Valid values are:
    /// - "user": Input provided by the user
    /// - "model": Response from the model
    /// - For system instructions, use "user" role
    pub(super) role: String,
}

/// A single part of content.
///
/// Represents an atomic piece of content, such as text or an image.
/// Currently only text is supported in this implementation.
#[derive(Debug, Deserialize, Serialize)]
pub(super) struct GooglePart {
    /// Text content of this part.
    ///
    /// Will be `None` for non-text content types (e.g., images, which are not yet supported).
    pub(super) text: Option<String>,
}

/// Response from Google Gemini GenerateContent API.
///
/// Contains the model's generated response along with metadata.
/// Documented in the [Google AI API Reference](https://ai.google.dev/api/generate-content#response-body).
#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
pub(super) struct GoogleGenerateResponse {
    /// List of response candidates from the model.
    ///
    /// Usually contains a single candidate unless `candidate_count` was set in the request.
    pub(super) candidates: Vec<GoogleCandidate>,

    /// Token usage metadata for this generation.
    ///
    /// Includes counts for input tokens, output tokens, and total.
    /// May be absent in some API responses.
    pub(super) usage_metadata: Option<GoogleUsageMetadata>,
}

/// A response candidate generated by the model.
///
/// Represents one possible response to the input prompt.
#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
pub(super) struct GoogleCandidate {
    /// Generated content from the model.
    pub(super) content: GoogleContent,

    /// The reason why the model stopped generating tokens.
    /// May be absent in streaming or partial responses.
    pub(super) finish_reason: Option<FinishReason>,

    /// Index of this candidate in the list of candidates.
    /// Defaults to 0 if not provided.
    #[serde(default)]
    pub(super) index: i32,

    /// Safety ratings for the generated content.
    ///
    /// Indicates probability of harmfulness across various categories.
    #[allow(dead_code)]
    pub(super) safety_ratings: Option<Vec<GoogleSafetyRating>>,
}

/// Safety rating for generated content.
///
/// Indicates the probability that content is harmful for a specific category.
#[derive(Debug, Deserialize)]
#[allow(dead_code)]
pub(super) struct GoogleSafetyRating {
    /// The category of potential harm.
    ///
    /// Categories include:
    /// - HARM_CATEGORY_HARASSMENT
    /// - HARM_CATEGORY_HATE_SPEECH  
    /// - HARM_CATEGORY_SEXUALLY_EXPLICIT
    /// - HARM_CATEGORY_DANGEROUS_CONTENT
    category: String,

    /// The probability level of harm.
    ///
    /// Values include:
    /// - NEGLIGIBLE: Negligible probability of harm
    /// - LOW: Low probability of harm
    /// - MEDIUM: Medium probability of harm
    /// - HIGH: High probability of harm
    probability: String,
}

/// Token usage statistics for a generation request.
///
/// Provides detailed token counts for billing and usage tracking.
#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
pub(super) struct GoogleUsageMetadata {
    /// Number of tokens in the prompt (input).
    pub(super) prompt_token_count: i32,

    /// Number of tokens in the generated response (output).
    pub(super) candidates_token_count: i32,

    /// Total number of tokens (prompt + candidates).
    pub(super) total_token_count: i32,
}

/// Response from listing available Gemini models.
///
/// Contains a list of models available for use with the Gemini API.
/// Documented in the [Google AI API Reference](https://ai.google.dev/api/models/list).
#[derive(Debug, Deserialize)]
pub(super) struct GoogleModelsResponse {
    /// List of available models.
    pub(super) models: Vec<GoogleModel>,

    /// Token for fetching the next page of results.
    ///
    /// If present, there are more models available that can be fetched
    /// by including this token in the next request.
    #[allow(dead_code)]
    pub(super) next_page_token: Option<String>,
}

/// Describes a Gemini model available for use.
///
/// Contains metadata about a specific model including its capabilities.
#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
pub(super) struct GoogleModel {
    /// The resource name of the model.
    ///
    /// Format: `models/{model}` where `{model}` is the model ID.
    /// Example: "models/gemini-1.5-pro"
    pub(super) name: String,

    /// List of generation methods supported by this model.
    ///
    /// Common values include:
    /// - "generateContent": Standard text generation
    /// - "streamGenerateContent": Streaming text generation
    /// - "countTokens": Token counting
    /// - "embedContent": Generate embeddings
    pub(super) supported_generation_methods: Vec<String>,
}

impl From<GoogleGenerateResponse> for ChatCompletionResponse {
    fn from(response: GoogleGenerateResponse) -> Self {
        let candidate = response
            .candidates
            .into_iter()
            .next()
            .expect("No candidates in Google response");

        let message_content = candidate
            .content
            .parts
            .iter()
            .filter_map(|part| part.text.as_ref())
            .cloned()
            .collect::<Vec<_>>()
            .join("");

        let finish_reason = candidate.finish_reason.as_ref().map_or_else(
            || {
                log::warn!("Google API response missing finish_reason, defaulting to 'stop'");
                crate::messages::FinishReason::Stop
            },
            |reason| match reason {
                FinishReason::Stop => crate::messages::FinishReason::Stop,
                FinishReason::MaxTokens => crate::messages::FinishReason::Length,
                FinishReason::Safety => crate::messages::FinishReason::ContentFilter,
                FinishReason::Recitation => crate::messages::FinishReason::ContentFilter,
                FinishReason::Other => crate::messages::FinishReason::Stop,
                FinishReason::Unknown(s) => {
                    log::warn!("Unknown finish reason from Google: {s}");
                    crate::messages::FinishReason::Other(s.clone())
                }
            },
        );

        Self {
            id: format!("gen-{}", uuid::Uuid::new_v4()),
            object: ObjectType::ChatCompletion,
            created: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap_or_default()
                .as_secs(),
            model: String::new(), // Will be set by the provider
            choices: vec![ChatChoice {
                index: candidate.index as u32,
                message: ChatMessage {
                    role: ChatRole::Assistant,
                    content: message_content,
                },
                finish_reason,
            }],
            usage: response.usage_metadata.map_or_else(
                || {
                    // If usage metadata is missing, provide default values
                    log::warn!("Google API response missing usage_metadata, using default values");
                    Usage {
                        prompt_tokens: 0,
                        completion_tokens: 0,
                        total_tokens: 0,
                    }
                },
                |metadata| Usage {
                    prompt_tokens: metadata.prompt_token_count as u32,
                    completion_tokens: metadata.candidates_token_count as u32,
                    total_tokens: metadata.total_token_count as u32,
                },
            ),
        }
    }
}

impl From<GoogleModel> for Model {
    fn from(model: GoogleModel) -> Self {
        // Extract model ID from the full name (e.g., "models/gemini-pro" -> "gemini-pro")
        let id = model.name.split('/').next_back().unwrap_or(&model.name).to_string();

        Self {
            id,
            object: ObjectType::Model,
            created: 0, // Google doesn't provide creation timestamps
            owned_by: "google".to_string(),
        }
    }
}
