//! AI21 output types for AWS Bedrock.

use crate::messages::{
    ChatChoice, ChatChoiceDelta, ChatCompletionChunk, ChatCompletionResponse, ChatMessage, ChatMessageDelta, ChatRole,
    FinishReason, ObjectType, Usage,
};
use serde::Deserialize;

/// Response from AI21 Jamba models.
///
/// The response format is similar to OpenAI's API format.
#[derive(Debug, Deserialize)]
pub(crate) struct JambaResponse {
    /// Unique identifier for the completion.
    pub id: String,

    /// Array of completion choices.
    pub choices: Vec<JambaChoice>,

    /// Token usage statistics.
    #[serde(default)]
    pub usage: Option<JambaUsage>,

    /// Unix timestamp of when the response was created.
    #[serde(default)]
    pub created: Option<i64>,

    /// The model that was used.
    #[serde(default)]
    pub model: Option<String>,
}

/// A single completion choice from Jamba.
#[derive(Debug, Deserialize)]
pub(crate) struct JambaChoice {
    /// The index of this choice.
    pub index: u32,

    /// The message generated by the model.
    pub message: JambaMessage,

    /// The reason the generation stopped.
    #[serde(default)]
    pub finish_reason: Option<JambaFinishReason>,
}

/// A message in the Jamba response.
#[derive(Debug, Deserialize)]
pub(crate) struct JambaMessage {
    /// The role of the message author.
    pub role: String,

    /// The content of the message.
    pub content: String,
}

/// Token usage statistics from Jamba.
#[derive(Debug, Deserialize)]
pub(crate) struct JambaUsage {
    /// Number of prompt/input tokens.
    pub prompt_tokens: u32,

    /// Number of completion/output tokens.
    pub completion_tokens: u32,

    /// Total tokens (prompt + completion).
    pub total_tokens: u32,
}

/// Jamba finish reason enum.
#[derive(Debug, Deserialize, Clone)]
#[serde(rename_all = "snake_case")]
pub(crate) enum JambaFinishReason {
    /// Natural completion.
    Stop,
    /// Maximum token limit reached.
    Length,
    /// Content filtering triggered.
    ContentFilter,
    /// Any other finish reason.
    #[serde(untagged)]
    Other(String),
}

impl From<JambaFinishReason> for FinishReason {
    fn from(reason: JambaFinishReason) -> Self {
        match reason {
            JambaFinishReason::Stop => FinishReason::Stop,
            JambaFinishReason::Length => FinishReason::Length,
            JambaFinishReason::ContentFilter => FinishReason::ContentFilter,
            JambaFinishReason::Other(s) => {
                log::warn!("Unknown finish reason from Jamba: {s}");
                FinishReason::Other(s)
            }
        }
    }
}

impl From<JambaResponse> for ChatCompletionResponse {
    fn from(response: JambaResponse) -> Self {
        let choices = response
            .choices
            .into_iter()
            .map(|choice| ChatChoice {
                index: choice.index,
                message: ChatMessage {
                    role: match choice.message.role.as_str() {
                        "system" => ChatRole::System,
                        "user" => ChatRole::User,
                        "assistant" => ChatRole::Assistant,
                        _ => ChatRole::Assistant,
                    },
                    content: choice.message.content,
                },
                finish_reason: choice
                    .finish_reason
                    .map(FinishReason::from)
                    .unwrap_or(FinishReason::Stop),
            })
            .collect();

        let usage = response.usage.map(|u| Usage {
            prompt_tokens: u.prompt_tokens,
            completion_tokens: u.completion_tokens,
            total_tokens: u.total_tokens,
        });

        Self {
            id: response.id,
            object: ObjectType::ChatCompletion,
            created: response.created.unwrap_or_else(|| {
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs() as i64
            }) as u64,
            model: response.model.unwrap_or_default(),
            choices,
            usage: usage.unwrap_or(Usage {
                prompt_tokens: 0,
                completion_tokens: 0,
                total_tokens: 0,
            }),
        }
    }
}

// Streaming support for Jamba models

/// AWS Bedrock invocation metrics that appear at the end of streaming.
#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
pub(crate) struct BedrockInvocationMetrics {
    /// Number of input tokens processed.
    pub input_token_count: u32,
    /// Number of output tokens generated.
    pub output_token_count: u32,
    /// Total invocation latency in milliseconds.
    #[allow(dead_code)]
    pub invocation_latency: u64,
    /// Time to first byte in milliseconds.
    #[allow(dead_code)]
    pub first_byte_latency: u64,
}

/// Metadata chunk from AWS Bedrock (appears at the end of streaming).
#[derive(Debug, Deserialize)]
pub(crate) struct BedrockMetadataChunk {
    /// Amazon Bedrock invocation metrics.
    #[serde(rename = "amazon-bedrock-invocationMetrics")]
    pub invocation_metrics: BedrockInvocationMetrics,
}

/// Enum to handle different types of streaming chunks from AI21.
#[derive(Debug, Deserialize)]
#[serde(untagged)]
pub(crate) enum AI21StreamEvent {
    /// Regular Jamba streaming chunk with content.
    JambaChunk(JambaStreamChunk),
    /// AWS Bedrock metadata chunk with metrics.
    Metadata(BedrockMetadataChunk),
}

/// Streaming chunk from Jamba models.
#[derive(Debug, Deserialize)]
pub(crate) struct JambaStreamChunk {
    /// Unique identifier for the chunk.
    pub id: String,

    /// Array of delta choices.
    pub choices: Vec<JambaStreamChoice>,

    /// Unix timestamp of when the chunk was created.
    #[serde(default)]
    pub created: Option<i64>,

    /// The model that was used.
    #[serde(default)]
    pub model: Option<String>,

    /// Usage statistics (only in final chunk).
    #[serde(default)]
    pub usage: Option<JambaUsage>,
}

/// A single choice in a streaming chunk.
#[derive(Debug, Deserialize)]
pub(crate) struct JambaStreamChoice {
    /// The index of this choice.
    pub index: u32,

    /// The delta content.
    pub delta: JambaStreamDelta,

    /// The reason the generation stopped (only in final chunk).
    #[serde(default)]
    pub finish_reason: Option<JambaFinishReason>,
}

/// Delta content in a streaming chunk.
#[derive(Debug, Deserialize)]
pub(crate) struct JambaStreamDelta {
    /// The role (only in first chunk).
    #[serde(default)]
    pub role: Option<String>,

    /// The incremental content.
    #[serde(default)]
    pub content: Option<String>,
}

impl From<AI21StreamEvent> for Option<ChatCompletionChunk> {
    fn from(event: AI21StreamEvent) -> Self {
        match event {
            AI21StreamEvent::JambaChunk(chunk) => {
                // Skip empty chunks
                if chunk.choices.is_empty() {
                    return None;
                }

                let choices = chunk
                    .choices
                    .into_iter()
                    .map(|choice| ChatChoiceDelta {
                        index: choice.index,
                        delta: ChatMessageDelta {
                            role: choice.delta.role.and_then(|r| match r.as_str() {
                                "system" => Some(ChatRole::System),
                                "user" => Some(ChatRole::User),
                                "assistant" => Some(ChatRole::Assistant),
                                _ => None,
                            }),
                            content: choice.delta.content,
                            function_call: None,
                            tool_calls: None,
                        },
                        finish_reason: choice.finish_reason.map(FinishReason::from),
                        logprobs: None,
                    })
                    .collect();

                let usage = chunk.usage.map(|u| Usage {
                    prompt_tokens: u.prompt_tokens,
                    completion_tokens: u.completion_tokens,
                    total_tokens: u.total_tokens,
                });

                Some(ChatCompletionChunk {
                    id: chunk.id,
                    object: ObjectType::ChatCompletionChunk,
                    created: chunk.created.unwrap_or_else(|| {
                        std::time::SystemTime::now()
                            .duration_since(std::time::UNIX_EPOCH)
                            .unwrap_or_default()
                            .as_secs() as i64
                    }) as u64,
                    model: chunk.model.unwrap_or_default(),
                    choices,
                    usage,
                    system_fingerprint: None,
                })
            }
            AI21StreamEvent::Metadata(metadata) => {
                // Create a final chunk with usage information from AWS Bedrock metrics
                Some(ChatCompletionChunk {
                    id: String::new(), // Will be filled by the caller
                    object: ObjectType::ChatCompletionChunk,
                    created: std::time::SystemTime::now()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap_or_default()
                        .as_secs(),
                    model: String::new(), // Will be filled by the caller
                    choices: vec![],      // Empty choices for metadata chunk
                    usage: Some(Usage {
                        prompt_tokens: metadata.invocation_metrics.input_token_count,
                        completion_tokens: metadata.invocation_metrics.output_token_count,
                        total_tokens: metadata.invocation_metrics.input_token_count
                            + metadata.invocation_metrics.output_token_count,
                    }),
                    system_fingerprint: None,
                })
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_jamba_response_parsing() {
        let response_json = r#"{
            "id": "chatcmpl-8zLI4FFBAAApK2mGJ1BJOrMrPZQ8N",
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": "Hello! I'm Jamba, an AI assistant. How can I help you today?"
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": 10,
                "completion_tokens": 15,
                "total_tokens": 25
            },
            "created": 1710000000,
            "model": "ai21.jamba-1.5-mini-v1:0"
        }"#;

        let parsed: JambaResponse = sonic_rs::from_str(response_json).expect("Failed to parse Jamba response");
        assert_eq!(parsed.id, "chatcmpl-8zLI4FFBAAApK2mGJ1BJOrMrPZQ8N");
        assert_eq!(parsed.choices.len(), 1);
        assert_eq!(
            parsed.choices[0].message.content,
            "Hello! I'm Jamba, an AI assistant. How can I help you today?"
        );
        assert!(parsed.usage.is_some());
        let usage = parsed.usage.unwrap();
        assert_eq!(usage.prompt_tokens, 10);
        assert_eq!(usage.completion_tokens, 15);
        assert_eq!(usage.total_tokens, 25);
    }

    #[test]
    fn test_jamba_stream_chunk_parsing() {
        let chunk_json = r#"{
            "id": "chatcmpl-8zLI4FFBAAApK2mGJ1BJOrMrPZQ8N",
            "choices": [
                {
                    "index": 0,
                    "delta": {
                        "content": "Hello"
                    }
                }
            ],
            "created": 1710000000,
            "model": "ai21.jamba-1.5-mini-v1:0"
        }"#;

        let parsed: JambaStreamChunk = sonic_rs::from_str(chunk_json).expect("Failed to parse stream chunk");
        assert_eq!(parsed.id, "chatcmpl-8zLI4FFBAAApK2mGJ1BJOrMrPZQ8N");
        assert_eq!(parsed.choices.len(), 1);
        assert_eq!(parsed.choices[0].delta.content, Some("Hello".to_string()));
    }

    #[test]
    fn test_jamba_final_stream_chunk() {
        let chunk_json = r#"{
            "id": "chatcmpl-8zLI4FFBAAApK2mGJ1BJOrMrPZQ8N",
            "choices": [
                {
                    "index": 0,
                    "delta": {},
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": 10,
                "completion_tokens": 15,
                "total_tokens": 25
            }
        }"#;

        let parsed: JambaStreamChunk = sonic_rs::from_str(chunk_json).expect("Failed to parse final chunk");
        assert!(parsed.choices[0].finish_reason.is_some());
        assert!(parsed.usage.is_some());
    }

    #[test]
    fn test_bedrock_metadata_chunk() {
        let metadata_json = r#"{
            "amazon-bedrock-invocationMetrics": {
                "inputTokenCount": 16,
                "outputTokenCount": 23,
                "invocationLatency": 434,
                "firstByteLatency": 120
            }
        }"#;

        let parsed: BedrockMetadataChunk = sonic_rs::from_str(metadata_json).expect("Failed to parse metadata chunk");
        assert_eq!(parsed.invocation_metrics.input_token_count, 16);
        assert_eq!(parsed.invocation_metrics.output_token_count, 23);
        assert_eq!(parsed.invocation_metrics.invocation_latency, 434);
        assert_eq!(parsed.invocation_metrics.first_byte_latency, 120);
    }

    #[test]
    fn test_ai21_stream_event_parsing() {
        // Test regular Jamba chunk
        let jamba_chunk_json = r#"{
            "id": "test-id",
            "choices": [{
                "index": 0,
                "delta": {"content": "Hello"}
            }]
        }"#;

        let event: AI21StreamEvent = sonic_rs::from_str(jamba_chunk_json).expect("Failed to parse Jamba chunk");
        match event {
            AI21StreamEvent::JambaChunk(chunk) => {
                assert_eq!(chunk.id, "test-id");
                assert_eq!(chunk.choices[0].delta.content, Some("Hello".to_string()));
            }
            AI21StreamEvent::Metadata(_) => {
                unreachable!("Expected JambaChunk variant but got Metadata")
            }
        }

        // Test metadata chunk
        let metadata_json = r#"{
            "amazon-bedrock-invocationMetrics": {
                "inputTokenCount": 10,
                "outputTokenCount": 20,
                "invocationLatency": 300,
                "firstByteLatency": 100
            }
        }"#;

        let event: AI21StreamEvent = sonic_rs::from_str(metadata_json).expect("Failed to parse metadata");
        match event {
            AI21StreamEvent::Metadata(metadata) => {
                assert_eq!(metadata.invocation_metrics.input_token_count, 10);
                assert_eq!(metadata.invocation_metrics.output_token_count, 20);
            }
            AI21StreamEvent::JambaChunk(_) => {
                unreachable!("Expected Metadata variant but got JambaChunk")
            }
        }
    }

    #[test]
    fn test_metadata_to_completion_chunk() {
        let metadata = BedrockMetadataChunk {
            invocation_metrics: BedrockInvocationMetrics {
                input_token_count: 16,
                output_token_count: 23,
                invocation_latency: 434,
                first_byte_latency: 120,
            },
        };

        let event = AI21StreamEvent::Metadata(metadata);
        let chunk: Option<ChatCompletionChunk> = event.into();

        assert!(chunk.is_some());
        let chunk = chunk.unwrap();
        assert!(chunk.usage.is_some());

        let usage = chunk.usage.unwrap();
        assert_eq!(usage.prompt_tokens, 16);
        assert_eq!(usage.completion_tokens, 23);
        assert_eq!(usage.total_tokens, 39);
    }
}
