use std::borrow::Cow;

use serde::Deserialize;

use crate::messages::{openai, unified};

/// Response from OpenAI Chat Completions API.
///
/// This struct represents the response format from the `/v1/chat/completions` endpoint
/// as documented in the [OpenAI API Reference](https://platform.openai.com/docs/api-reference/chat/object).
#[derive(Debug, Deserialize)]
pub struct OpenAIResponse {
    /// A unique identifier for the chat completion.
    pub(super) id: String,

    /// The object type, which is always `chat.completion`.
    #[allow(dead_code)]
    pub(super) object: String,

    /// The Unix timestamp (in seconds) of when the chat completion was created.
    pub(super) created: u64,

    /// A list of chat completion choices.
    /// Can be more than one if `n` is greater than 1 in the request.
    pub(super) choices: Vec<OpenAIChoice>,

    /// Usage statistics for the completion request.
    /// Includes information about tokens used for prompt, completion, and total.
    pub(super) usage: openai::Usage,
}

/// The reason why OpenAI stopped generating tokens.
#[derive(Debug, Deserialize, PartialEq)]
pub enum OpenAIFinishReason {
    /// Natural stop point or stop sequence encountered.
    #[serde(rename = "stop")]
    Stop,
    /// Maximum token limit reached.
    #[serde(rename = "length")]
    Length,
    /// Content filtered for safety/policy reasons.
    #[serde(rename = "content_filter")]
    ContentFilter,
    /// Model invoked a tool/function.
    #[serde(rename = "tool_calls")]
    ToolCalls,
    /// Model invoked a function (legacy).
    #[serde(rename = "function_call")]
    FunctionCall,
    /// Any other finish reason.
    /// Captures the actual string value for forward compatibility.
    #[serde(untagged)]
    Other(String),
}

/// A chat completion choice returned by the model.
///
/// Represents one possible completion for the given input.
/// Multiple choices can be returned if requested via the `n` parameter.
#[derive(Debug, Deserialize)]
#[serde(rename_all = "snake_case")]
pub struct OpenAIChoice {
    /// The index of this choice in the list of choices.
    pub(super) index: u32,

    /// A chat completion message generated by the model.
    pub(super) message: openai::ChatMessage,

    /// The reason the model stopped generating tokens.
    pub(super) finish_reason: Option<OpenAIFinishReason>,
}

impl From<OpenAIFinishReason> for openai::FinishReason {
    fn from(reason: OpenAIFinishReason) -> Self {
        match reason {
            OpenAIFinishReason::Stop => openai::FinishReason::Stop,
            OpenAIFinishReason::Length => openai::FinishReason::Length,
            OpenAIFinishReason::ContentFilter => openai::FinishReason::ContentFilter,
            OpenAIFinishReason::ToolCalls | OpenAIFinishReason::FunctionCall => openai::FinishReason::ToolCalls,
            OpenAIFinishReason::Other(s) => {
                log::debug!("Unknown finish reason from OpenAI: {s}");
                openai::FinishReason::Other(s)
            }
        }
    }
}

impl From<OpenAIChoice> for openai::ChatChoice {
    fn from(choice: OpenAIChoice) -> Self {
        Self {
            index: choice.index,
            message: choice.message,
            finish_reason: choice
                .finish_reason
                .map(Into::into)
                .unwrap_or(openai::FinishReason::Stop),
        }
    }
}

impl From<OpenAIResponse> for unified::UnifiedResponse {
    fn from(response: OpenAIResponse) -> Self {
        let usage = response.usage;

        let choices = response
            .choices
            .into_iter()
            .map(|choice| {
                let finish_reason = choice.finish_reason.map(|reason| {
                    let openai_reason: openai::FinishReason = reason.into();
                    unified::UnifiedFinishReason::from(openai_reason)
                });

                unified::UnifiedChoice {
                    index: choice.index,
                    message: unified::UnifiedMessage::from(choice.message),
                    finish_reason,
                }
            })
            .collect();

        unified::UnifiedResponse {
            id: response.id,
            model: String::new(), // Provider fills in the model identifier later
            choices,
            usage: unified::UnifiedUsage {
                prompt_tokens: usage.prompt_tokens,
                completion_tokens: usage.completion_tokens,
                total_tokens: usage.total_tokens,
            },
            created: response.created,
            stop_reason: None,
            stop_sequence: None,
        }
    }
}

// Streaming types for OpenAI SSE responses

/// OpenAI streaming chunk format with borrowed strings for zero-copy parsing.
///
/// This represents a single Server-Sent Event in OpenAI's streaming response format.
/// Each chunk contains incremental updates to the chat completion response.
///
/// See: https://platform.openai.com/docs/api-reference/chat/streaming
///
/// This struct uses lifetime 'a to borrow strings directly from the input buffer,
/// avoiding allocations during JSON parsing for better performance.
#[derive(Debug, Deserialize)]
pub struct OpenAIStreamChunk<'a> {
    /// Unique identifier for this completion stream.
    ///
    /// Format: "chatcmpl-{alphanumeric}"
    /// Example: "chatcmpl-123abc"
    /// This ID is consistent across all chunks in the same stream.
    pub(super) id: Cow<'a, str>,

    /// The object type, always "chat.completion.chunk" for streaming responses.
    ///
    /// This differentiates streaming chunks from regular completion responses
    /// which have object type "chat.completion".
    #[allow(dead_code)]
    pub(super) object: Cow<'a, str>,

    /// Unix timestamp (seconds since epoch) when this chunk was created.
    ///
    /// All chunks in the same stream typically have the same timestamp.
    pub(super) created: u64,

    /// The model that generated this completion.
    ///
    /// Examples: "gpt-4", "gpt-3.5-turbo", "gpt-4-turbo-preview"
    /// Note: This is the model name without provider prefix.
    pub(super) model: Cow<'a, str>,

    /// Array of choice deltas containing the incremental content.
    ///
    /// Usually contains a single choice (index 0) unless n > 1 was specified
    /// in the request. Each choice represents a different completion path.
    pub(super) choices: Vec<OpenAIStreamChoice<'a>>,

    /// System fingerprint for reproducibility and debugging.
    ///
    /// Format: "fp_{alphanumeric}"
    /// Example: "fp_44709d6fcb"
    /// This helps OpenAI track the exact model configuration used.
    /// Only present in some models and API versions.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(super) system_fingerprint: Option<Cow<'a, str>>,

    /// Token usage statistics.
    ///
    /// Only present in the final chunk of the stream (when finish_reason is set).
    /// Contains prompt_tokens, completion_tokens, and total_tokens counts.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(super) usage: Option<openai::Usage>,
}

/// A streaming choice delta from OpenAI.
///
/// Represents incremental updates to a single completion choice.
/// In streaming mode, the content is delivered incrementally through
/// multiple chunks rather than all at once.
#[derive(Debug, Deserialize)]
pub struct OpenAIStreamChoice<'a> {
    /// Zero-based index of this choice in the choices array.
    ///
    /// Used to track multiple parallel completions when n > 1.
    /// Most requests have only one choice (index: 0).
    pub(super) index: u32,

    /// The incremental content update for this choice.
    ///
    /// Contains partial message content that should be appended
    /// to build the complete response.
    pub(super) delta: OpenAIDelta<'a>,

    /// The reason why the model stopped generating tokens.
    ///
    /// Only present in the final chunk for this choice.
    /// Possible values:
    /// - "stop": Natural completion or stop sequence reached
    /// - "length": Maximum token limit reached
    /// - "content_filter": Content was filtered
    /// - "tool_calls": Model decided to call a function/tool
    /// - "function_call": (Deprecated) Model called a function
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(super) finish_reason: Option<Cow<'a, str>>,

    /// Log probability information for the tokens in this chunk.
    ///
    /// Only present if `logprobs: true` was set in the request.
    /// Contains detailed token-level probability information including:
    /// - tokens: The tokens generated
    /// - token_logprobs: Log probabilities for each token
    /// - top_logprobs: Alternative tokens and their probabilities
    /// - text_offset: Character offsets in the text
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(super) logprobs: Option<sonic_rs::OwnedLazyValue>,
}

/// Delta content in a streaming response.
///
/// Contains the incremental updates to the message being constructed.
/// Fields are optional because different chunks contain different updates:
/// - First chunk: Usually contains role
/// - Middle chunks: Contain content text fragments
/// - Tool call chunks: Contain tool_calls array updates
/// - Final chunk: Often empty (finish_reason is in the choice)
#[derive(Debug, Deserialize)]
pub struct OpenAIDelta<'a> {
    /// The role of the message author.
    ///
    /// Only present in the first chunk of a message.
    /// Always "assistant" for model responses.
    /// Possible values: "system", "user", "assistant", "tool"
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(super) role: Option<Cow<'a, str>>,

    /// Incremental text content to append to the message.
    ///
    /// Contains a fragment of text that should be concatenated with
    /// previous content chunks to build the complete message.
    /// Can be an empty string, especially in first/last chunks.
    /// Will be None when the model is calling tools instead of responding with text.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(super) content: Option<Cow<'a, str>>,

    /// Incremental updates to tool/function calls.
    ///
    /// When the model decides to call tools, this array contains incremental
    /// updates to build the complete tool call. Each item represents a tool
    /// being called with:
    /// - index: Position in the tool calls array
    /// - id: Unique identifier for this tool call (first chunk only)
    /// - type: "function" (first chunk only)
    /// - function.name: Name of the function (first chunk only)
    /// - function.arguments: JSON arguments as a string (accumulated over chunks)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(super) tool_calls: Option<Vec<sonic_rs::OwnedLazyValue>>,

    /// Legacy function calling format (deprecated).
    ///
    /// Deprecated in favor of tool_calls but still supported for backward compatibility.
    /// Contains incremental updates to a function call with:
    /// - name: Function name (first chunk only)
    /// - arguments: JSON arguments as a string (accumulated over chunks)
    ///
    /// New integrations should use tool_calls instead.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(super) function_call: Option<sonic_rs::OwnedLazyValue>,
}

impl<'a> From<OpenAIDelta<'a>> for openai::ChatMessageDelta {
    fn from(delta: OpenAIDelta<'a>) -> Self {
        use crate::messages::openai::{StreamingFunctionCall, StreamingToolCall};

        // Convert tool_calls from sonic_rs::Value to StreamingToolCall
        let tool_calls = delta.tool_calls.map(|calls| {
            calls
                .into_iter()
                .filter_map(|call| {
                    // Try to deserialize the Value into our enum
                    sonic_rs::from_str::<StreamingToolCall>(&sonic_rs::to_string(&call).unwrap_or_default()).ok()
                })
                .collect()
        });

        // Convert function_call from sonic_rs::Value to StreamingFunctionCall
        let function_call = delta.function_call.and_then(|call| {
            sonic_rs::from_str::<StreamingFunctionCall>(&sonic_rs::to_string(&call).unwrap_or_default()).ok()
        });

        Self {
            role: delta.role.map(|s| match s.as_ref() {
                "assistant" => openai::ChatRole::Assistant,
                "user" => openai::ChatRole::User,
                "system" => openai::ChatRole::System,
                other => openai::ChatRole::Other(other.to_string()),
            }),
            content: delta.content.map(|s| s.into_owned()),
            tool_calls,
            function_call,
        }
    }
}

impl<'a> From<OpenAIStreamChoice<'a>> for openai::ChatChoiceDelta {
    fn from(choice: OpenAIStreamChoice<'a>) -> Self {
        Self {
            index: choice.index,
            delta: choice.delta.into(),
            finish_reason: choice.finish_reason.map(|s| {
                // Parse finish reason string to enum
                match s.as_ref() {
                    "stop" => openai::FinishReason::Stop,
                    "length" => openai::FinishReason::Length,
                    "content_filter" => openai::FinishReason::ContentFilter,
                    "tool_calls" | "function_call" => openai::FinishReason::ToolCalls,
                    other => openai::FinishReason::Other(other.to_string()),
                }
            }),
            logprobs: choice.logprobs,
        }
    }
}

impl<'a> OpenAIStreamChunk<'a> {
    /// Convert to UnifiedChunk with provider name prefix.
    pub fn into_chunk(self, provider_name: &str) -> unified::UnifiedChunk {
        // First convert to OpenAI format
        let openai_chunk = openai::ChatCompletionChunk {
            id: self.id.into_owned(),
            object: openai::ObjectType::ChatCompletionChunk,
            created: self.created,
            model: format!("{provider_name}/{}", self.model),
            choices: self.choices.into_iter().map(Into::into).collect(),
            system_fingerprint: self.system_fingerprint.map(|s| s.into_owned()),
            usage: self.usage,
        };

        // Then convert to unified format
        unified::UnifiedChunk::from(openai_chunk)
    }
}
