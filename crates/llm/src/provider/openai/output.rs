use std::borrow::Cow;

use serde::Deserialize;

use crate::messages::{openai, unified};

/// Response from OpenAI Chat Completions API.
///
/// This struct represents the response format from the `/v1/chat/completions` endpoint
/// as documented in the [OpenAI API Reference](https://platform.openai.com/docs/api-reference/chat/object).
#[derive(Debug, Deserialize)]
pub struct OpenAIResponse {
    /// A unique identifier for the chat completion.
    pub(super) id: String,

    /// The object type, which is always `chat.completion`.
    #[allow(dead_code)]
    pub(super) object: String,

    /// The Unix timestamp (in seconds) of when the chat completion was created.
    pub(super) created: u64,

    /// A list of chat completion choices.
    /// Can be more than one if `n` is greater than 1 in the request.
    pub(super) choices: Vec<OpenAIChoice>,

    /// Usage statistics for the completion request.
    /// Includes information about tokens used for prompt, completion, and total.
    pub(super) usage: openai::Usage,
}

/// The reason why OpenAI stopped generating tokens.
#[derive(Debug, Deserialize, PartialEq)]
pub enum OpenAIFinishReason {
    /// Natural stop point or stop sequence encountered.
    #[serde(rename = "stop")]
    Stop,
    /// Maximum token limit reached.
    #[serde(rename = "length")]
    Length,
    /// Content filtered for safety/policy reasons.
    #[serde(rename = "content_filter")]
    ContentFilter,
    /// Model invoked a tool/function.
    #[serde(rename = "tool_calls")]
    ToolCalls,
    /// Model invoked a function (legacy).
    #[serde(rename = "function_call")]
    FunctionCall,
    /// Any other finish reason.
    /// Captures the actual string value for forward compatibility.
    #[serde(untagged)]
    Other(String),
}

/// A chat completion choice returned by the model.
///
/// Represents one possible completion for the given input.
/// Multiple choices can be returned if requested via the `n` parameter.
#[derive(Debug, Deserialize)]
#[serde(rename_all = "snake_case")]
pub struct OpenAIChoice {
    /// The index of this choice in the list of choices.
    pub(super) index: u32,

    /// A chat completion message generated by the model.
    pub(super) message: openai::ChatMessage,

    /// The reason the model stopped generating tokens.
    pub(super) finish_reason: Option<OpenAIFinishReason>,
}

impl From<OpenAIFinishReason> for unified::UnifiedFinishReason {
    fn from(reason: OpenAIFinishReason) -> Self {
        match reason {
            OpenAIFinishReason::Stop => unified::UnifiedFinishReason::Stop,
            OpenAIFinishReason::Length => unified::UnifiedFinishReason::Length,
            OpenAIFinishReason::ToolCalls | OpenAIFinishReason::FunctionCall => unified::UnifiedFinishReason::ToolCalls,
            OpenAIFinishReason::ContentFilter => unified::UnifiedFinishReason::ContentFilter,
            // Map any other finish reason to Stop as a safe default
            OpenAIFinishReason::Other(_) => unified::UnifiedFinishReason::Stop,
        }
    }
}

impl From<OpenAIResponse> for unified::UnifiedResponse {
    fn from(response: OpenAIResponse) -> Self {
        let usage = response.usage;

        let choices = response
            .choices
            .into_iter()
            .map(|choice| {
                let finish_reason = choice.finish_reason.map(unified::UnifiedFinishReason::from);

                unified::UnifiedChoice {
                    index: choice.index,
                    message: unified::UnifiedMessage::from(choice.message),
                    finish_reason,
                }
            })
            .collect();

        unified::UnifiedResponse {
            id: response.id,
            model: String::new(), // Provider fills in the model identifier later
            choices,
            usage: unified::UnifiedUsage {
                prompt_tokens: usage.prompt_tokens,
                completion_tokens: usage.completion_tokens,
                total_tokens: usage.total_tokens,
            },
            created: response.created,
            stop_reason: None,
            stop_sequence: None,
        }
    }
}

// Streaming types for OpenAI SSE responses

/// OpenAI streaming chunk format with borrowed strings for zero-copy parsing.
///
/// This represents a single Server-Sent Event in OpenAI's streaming response format.
/// Each chunk contains incremental updates to the chat completion response.
///
/// See: https://platform.openai.com/docs/api-reference/chat/streaming
///
/// This struct uses lifetime 'a to borrow strings directly from the input buffer,
/// avoiding allocations during JSON parsing for better performance.
#[derive(Debug, Deserialize)]
pub struct OpenAIStreamChunk<'a> {
    /// Unique identifier for this completion stream.
    ///
    /// Format: "chatcmpl-{alphanumeric}"
    /// Example: "chatcmpl-123abc"
    /// This ID is consistent across all chunks in the same stream.
    pub(super) id: Cow<'a, str>,

    /// The object type, always "chat.completion.chunk" for streaming responses.
    ///
    /// This differentiates streaming chunks from regular completion responses
    /// which have object type "chat.completion".
    #[allow(dead_code)]
    pub(super) object: Cow<'a, str>,

    /// Unix timestamp (seconds since epoch) when this chunk was created.
    ///
    /// All chunks in the same stream typically have the same timestamp.
    pub(super) created: u64,

    /// The model that generated this completion.
    ///
    /// Examples: "gpt-4", "gpt-3.5-turbo", "gpt-4-turbo-preview"
    /// Note: This is the model name without provider prefix.
    pub(super) model: Cow<'a, str>,

    /// Array of choice deltas containing the incremental content.
    ///
    /// Usually contains a single choice (index 0) unless n > 1 was specified
    /// in the request. Each choice represents a different completion path.
    pub(super) choices: Vec<OpenAIStreamChoice<'a>>,

    /// System fingerprint for reproducibility and debugging.
    ///
    /// Format: "fp_{alphanumeric}"
    /// Example: "fp_44709d6fcb"
    /// This helps OpenAI track the exact model configuration used.
    /// Only present in some models and API versions.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[allow(dead_code)]
    pub(super) system_fingerprint: Option<Cow<'a, str>>,

    /// Token usage statistics.
    ///
    /// Only present in the final chunk of the stream (when finish_reason is set).
    /// Contains prompt_tokens, completion_tokens, and total_tokens counts.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(super) usage: Option<openai::Usage>,
}

/// A streaming choice delta from OpenAI.
///
/// Represents incremental updates to a single completion choice.
/// In streaming mode, the content is delivered incrementally through
/// multiple chunks rather than all at once.
#[derive(Debug, Deserialize)]
pub struct OpenAIStreamChoice<'a> {
    /// Zero-based index of this choice in the choices array.
    ///
    /// Used to track multiple parallel completions when n > 1.
    /// Most requests have only one choice (index: 0).
    pub(super) index: u32,

    /// The incremental content update for this choice.
    ///
    /// Contains partial message content that should be appended
    /// to build the complete response.
    pub(super) delta: OpenAIDelta<'a>,

    /// The reason why the model stopped generating tokens.
    ///
    /// Only present in the final chunk for this choice.
    /// Possible values:
    /// - "stop": Natural completion or stop sequence reached
    /// - "length": Maximum token limit reached
    /// - "content_filter": Content was filtered
    /// - "tool_calls": Model decided to call a function/tool
    /// - "function_call": (Deprecated) Model called a function
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(super) finish_reason: Option<Cow<'a, str>>,

    /// Log probability information for the tokens in this chunk.
    ///
    /// Only present if `logprobs: true` was set in the request.
    /// Contains detailed token-level probability information including:
    /// - tokens: The tokens generated
    /// - token_logprobs: Log probabilities for each token
    /// - top_logprobs: Alternative tokens and their probabilities
    /// - text_offset: Character offsets in the text
    #[serde(skip_serializing_if = "Option::is_none")]
    #[allow(dead_code)]
    pub(super) logprobs: Option<sonic_rs::OwnedLazyValue>,
}

/// Delta content in a streaming response.
///
/// Contains the incremental updates to the message being constructed.
/// Fields are optional because different chunks contain different updates:
/// - First chunk: Usually contains role
/// - Middle chunks: Contain content text fragments
/// - Tool call chunks: Contain tool_calls array updates
/// - Final chunk: Often empty (finish_reason is in the choice)
#[derive(Debug, Deserialize)]
pub struct OpenAIDelta<'a> {
    /// The role of the message author.
    ///
    /// Only present in the first chunk of a message.
    /// Always "assistant" for model responses.
    /// Possible values: "system", "user", "assistant", "tool"
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(super) role: Option<Cow<'a, str>>,

    /// Incremental text content to append to the message.
    ///
    /// Contains a fragment of text that should be concatenated with
    /// previous content chunks to build the complete message.
    /// Can be an empty string, especially in first/last chunks.
    /// Will be None when the model is calling tools instead of responding with text.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(super) content: Option<Cow<'a, str>>,

    /// Incremental updates to tool/function calls.
    ///
    /// When the model decides to call tools, this array contains incremental
    /// updates to build the complete tool call. Each item represents a tool
    /// being called with:
    /// - index: Position in the tool calls array
    /// - id: Unique identifier for this tool call (first chunk only)
    /// - type: "function" (first chunk only)
    /// - function.name: Name of the function (first chunk only)
    /// - function.arguments: JSON arguments as a string (accumulated over chunks)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub(super) tool_calls: Option<Vec<sonic_rs::OwnedLazyValue>>,

    /// Legacy function calling format (deprecated).
    ///
    /// Deprecated in favor of tool_calls but still supported for backward compatibility.
    /// Contains incremental updates to a function call with:
    /// - name: Function name (first chunk only)
    /// - arguments: JSON arguments as a string (accumulated over chunks)
    ///
    /// New integrations should use tool_calls instead.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[allow(dead_code)]
    pub(super) function_call: Option<sonic_rs::OwnedLazyValue>,
}

impl<'a> From<OpenAIStreamChoice<'a>> for unified::UnifiedChoiceDelta {
    fn from(choice: OpenAIStreamChoice<'a>) -> Self {
        use crate::messages::openai::StreamingToolCall;

        // Convert tool_calls from OwnedLazyValue to StreamingToolCall
        let tool_calls = choice.delta.tool_calls.map(|calls| {
            calls
                .into_iter()
                .filter_map(|call| {
                    // Deserialize from OwnedLazyValue - requires serialization
                    sonic_rs::to_string(&call)
                        .ok()
                        .and_then(|s| sonic_rs::from_str::<StreamingToolCall>(&s).ok())
                })
                .map(unified::UnifiedStreamingToolCall::from)
                .collect()
        });

        // Handle finish reason
        let finish_reason = choice.finish_reason.and_then(|reason| {
            match reason.as_ref() {
                "stop" => Some(unified::UnifiedFinishReason::Stop),
                "length" => Some(unified::UnifiedFinishReason::Length),
                "tool_calls" | "function_call" => Some(unified::UnifiedFinishReason::ToolCalls),
                "content_filter" => Some(unified::UnifiedFinishReason::ContentFilter),
                // Map unknown finish reasons to Stop as a safe default
                other if !other.is_empty() => {
                    log::debug!("Unknown OpenAI finish reason: {}", other);
                    Some(unified::UnifiedFinishReason::Stop)
                }
                _ => None,
            }
        });

        unified::UnifiedChoiceDelta {
            index: choice.index,
            delta: unified::UnifiedMessageDelta {
                role: choice.delta.role.map(|r| match r.as_ref() {
                    "assistant" => unified::UnifiedRole::Assistant,
                    "user" => unified::UnifiedRole::User,
                    "system" => unified::UnifiedRole::System,
                    "tool" => unified::UnifiedRole::Tool,
                    _ => unified::UnifiedRole::Assistant,
                }),
                content: choice.delta.content.map(|c| c.into_owned()),
                tool_calls,
            },
            finish_reason,
        }
    }
}

impl<'a> OpenAIStreamChunk<'a> {
    /// Convert to UnifiedChunk with provider name prefix.
    pub fn into_chunk(self, provider_name: &str) -> unified::UnifiedChunk {
        unified::UnifiedChunk {
            id: self.id.into_owned().into(),
            model: format!("{provider_name}/{}", self.model).into(),
            choices: self
                .choices
                .into_iter()
                .map(unified::UnifiedChoiceDelta::from)
                .collect(),
            usage: self.usage.map(|u| unified::UnifiedUsage {
                prompt_tokens: u.prompt_tokens,
                completion_tokens: u.completion_tokens,
                total_tokens: u.total_tokens,
            }),
            created: self.created,
        }
    }
}
